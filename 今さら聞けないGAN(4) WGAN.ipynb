{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 今さら聞けないGANシリーズ(4)\n",
    "https://qiita.com/triwave33/items/5c95db572b0e4d0df4f0  \n",
    "Wasserstein GANについて"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wasserstein距離で損失関数を設計したWGANのコンセプトを学ぶ  \n",
    "さらにGradient penaltyを導入したWGAN-gpを導入する"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 損失関数\n",
    "通常のGANの損失関数  \n",
    "$$ min_G max_D V(D, G) = \\mathbb{E}_{x \\sim p_{data}(x)} [ \\log{D(x)} ] + \\mathbb{E}_{z \\sim p_z(z)} [ \\log{(1 - D(G(z))} ] $$\n",
    "\n",
    "Discriminatorを最適化する時は，本物データを本物と識別し，偽物データを偽物と識別するのが良い．  \n",
    "これは二値分類問題なので，binary cross entropy関数を損失関数として用いていた．  \n",
    "  \n",
    "本物データの確率密度分布$p_{data}(x)$と生成データの確率密度分布$p_g(z)$が固定されている場合，最適な識別関数$D^*$は以下になる．($x=G(z)$)  \n",
    "$$ D^*(x) = \\frac{p_{data}(x)}{p_{data}(x) + p_g(x)}$$\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
