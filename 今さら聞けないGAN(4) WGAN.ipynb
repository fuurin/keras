{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 今さら聞けないGANシリーズ(4)\n",
    "https://qiita.com/triwave33/items/5c95db572b0e4d0df4f0  \n",
    "Wasserstein GANについて  \n",
    "\n",
    "https://arxiv.org/pdf/1701.07875.pdf  \n",
    "WGANの原論文  \n",
    "論文発表に使ったり，参考文献にしたり  \n",
    "\n",
    "https://www.alexirpan.com/2017/02/22/wasserstein-gan.html  \n",
    "⬆︎の素晴らしい概要\n",
    "  \n",
    "http://musyoku.github.io/2017/02/06/Wasserstein-GAN/  \n",
    "WGANの簡単な理論と実装について，アニメ画像生成で実験  \n",
    "  \n",
    "http://yusuke-ujitoko.hatenablog.com/entry/2017/05/20/145924  \n",
    "WGANの⬆︎よりは詳しい理論といらすとや画像生成による実験  \n",
    "  \n",
    "https://qiita.com/mittyantest/items/0fdc9ce7624dbd2ee134  \n",
    "WGANとKantorovich-Rubinstein双対性  \n",
    "Wasserstein距離についての数学的な説明，むじゅい"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wasserstein距離で損失関数を設計したWGANのコンセプトを学ぶ  \n",
    "さらにGradient penaltyを導入したWGAN-gpを導入する"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## まとめ\n",
    "- GANはGeneratorのパラメータの最適値付近で勾配消失し学習が進まなくなる\n",
    "    - 損失関数のbinary cross entropy関数がJSDを使っていることになるため\n",
    "- JSDの代わりにWasserstein距離を損失関数に使う**WGAN**を提唱\n",
    "    - 結果，損失関数にはlogを用いない\n",
    "    - D(x)の結果にsigmoidをかける必要がなくなった\n",
    "- WGANではD(x)がリプシッツな関数である必要がある\n",
    "- リプシッツな関数にするためにパラメータのclipをしていたが，不安定\n",
    "- clipの代わりに勾配ペナルティを損失関数に加えた**WGAN-gp**を提唱\n",
    "    - 生成データと実データの間の点では勾配のL2ノルムが1になる\n",
    "    - そのL2ノルムが1にならない時ペナルティを与え，リプシッツにする\n",
    "    - 実装では，f-imgとr-imgから得るWGANのlossとそれらの間のa-imgから得るgradient penaltyからなる損失関数を最適化する"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 損失関数\n",
    "通常のGANの損失関数  \n",
    "$$ min_G max_D V(D, G) = \\mathbb{E}_{x \\sim p_{data}(x)} [ \\log{D(x)} ] + \\mathbb{E}_{z \\sim p_z(z)} [ \\log{(1 - D(G(z))} ] $$\n",
    "\n",
    "Discriminatorを最適化する時は，本物データを本物と識別し，偽物データを偽物と識別するのが良い．  \n",
    "これは二値分類問題なので，binary cross entropy関数を損失関数として用いていた．  \n",
    "  \n",
    "本物データの確率密度分布$p_{data}(x)$と生成データの確率密度分布$p_g(z)$が固定されている場合(Dはこれら2つを持っており，それを学習する?)，最適な識別関数$D^*$は以下になる．($x=G(z)$または本物データ)  \n",
    "$$ D^*(x) = \\frac{p_{data}(x)}{p_{data}(x) + p_g(x)}$$\n",
    "\n",
    "直感的には，何らかの画像データを入力した時に本物データの分布に近い出力を得た時1(正解)，そうでないとき，つまり偽物データの分布に近い出力を得た時0(不正解)を出力する関数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jensen-Shannon ダイバージェンス  \n",
    "最適なDiscriminator下でのGeneratorの価値関数は次のようになる  \n",
    "  \n",
    "$$ C(G) = -log(4) + 2 \\times JSD(p_{data} || p_g) $$ \n",
    "  \n",
    "JSDはJensen-Shannon ダイバージェンスで2つの確率密度間の距離を示す．  \n",
    "$p_{data}(x)$と$p_z(x)$が全ての$x$で完全に一致する時，JSDは0になる．  \n",
    "  \n",
    "言い換えると，通常のGANはJSDを指標に，2つの確率密度間の距離を近づけていく作業  \n",
    "  \n",
    "しかしこれによって，勾配消失問題が発生してしまう．  \n",
    "Generatorのパラメータ$\\theta$の最適値周りで勾配が0になってしまい，学習がうまく行かなくなることが知られている．(WGANの原論文より)  \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "そこで，JSDの代わりにWasserstein距離を使うというコンセプトに基づくのがWGAN  \n",
    "WGANはパラメータの最適点付近で勾配が消失せず，学習が安定して進む  \n",
    "結果だけ書くと，Wasserstein距離Wは以下のように表される  \n",
    "  \n",
    "$$ W(\\mathbb{P}_r, \\mathbb{P}_\\theta) = max_{w \\in W} \\mathbb{E}_{x \\sim \\mathbb{P}_r} [f_W (x)] - \\mathbb{E}_{z \\sim p(z)}[f_W(g_{\\theta}(z))] $$\n",
    "  \n",
    "kerasでは損失関数の最小化を行うので，この式にマイナスをかけて最小化問題に定式化する．結果，最小化するべき損失関数$L$は\n",
    "  \n",
    "$$ L = \\mathbb{E}_{\\tilde{x} \\sim \\mathbb{P}_g} [D(\\tilde{x})] - \\mathbb{E}_{\\tilde{x} \\sim \\mathbb{P}_r}  [D(x)] $$\n",
    "  \n",
    "$\\tilde{x}$はzより生成した画像，$x$は本物の画像を示す．  \n",
    "  \n",
    "WGANの特徴は，**損失関数にlogを用いていない**ことである．  \n",
    "さらに$D(x)$は識別結果としての意味を持たず，W距離計算のためのパーツになったため，出力をsigmoid関数で[0, 1]に押し込める必要もない．  \n",
    "そのため，WGANでは$D(x)$のことを$f(x)$と表したり，Discriminatorの代わりにCriticと呼んだりする．  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discriminatorの制約条件\n",
    "$D(x)$がWasserstein距離として意味を持つには，$D(x)$がリプシッツな関数である必要がある．  \n",
    "詳しいことは置いておくが，この制約条件を満たすために，初期のWGANでは重みパラメータの最小，最大値をclipしているが，これが不安定になる要因になっている．  \n",
    "そこで，clipの代わりに損失関数にペナルティ項を与えることで学習の最適化を達成する，WGAN-gpというものが提案された．  \n",
    "  \n",
    "最適化されたDiscriminatorでは，生成データと本物データ間の任意の点での勾配のL2ノルムが1になるという性質があることが知られている．  \n",
    "この生死を逆手にとって，損失関数に勾配のL2ノルムが1以外の時にペナルティを課す  \n",
    "\n",
    "$$ L_{WGANgp} = L_{WGAN} + \\lambda \\mathbb{E}_{\\hat{x} \\sim \\mathbb{P}_{\\hat{x}}} [ (||\\nabla_{\\hat{x}} D(\\hat{x})||_2 - 1)^2 ] $$\n",
    "ここで$\\hat{x}$は生成データと本物データを結んだ直線上の任意の点  \n",
    "  \n",
    "これでリプシッツな関数になるということ？  \n",
    "つまり，生成データと本物データの外側に行かないように学習する？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WGAN-gpのDiscriminatorの構造\n",
    "通常のGANではGeneratorとDiscriminatorを完全に切り離していた．  \n",
    "また，本物データと生成データは別々に学習させる  \n",
    "  \n",
    "WGAN-gpのDiscriminatorは  \n",
    "- 本物データと生成データを同時に学習させる  \n",
    "- WGAN-gpの入力はnoizeとr-imgを用いる\n",
    "- Discriminatorに対する実質的な入力は\n",
    "    - f-img (fake)\n",
    "    - r-img (real)\n",
    "    - a-img (average, 各入力を直線で繋いだ間の任意の点)\n",
    "        - a-imgの生成のため，RandomWeightedAverageを実装\n",
    "- これらの画像をDに与え，f-out,r-out,a-outを得て損失関数の値とする\n",
    "    - f-outとr-outから得るOriginal crigtic loss\n",
    "    - a-outから得るgradient penalty\n",
    "- 最終的にOptimizerを定義することによってDiscriminatorの学習を進める"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GANを試す場合は，うまく行かないのがアルゴリズムのせいか，実装のせいか，パラメータのせいか，どれが原因かわかりにくいのが問題"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
